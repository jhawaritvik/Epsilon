import os
import json
import logging
from pathlib import Path
from typing import Dict, Any, List

# Try to import OpenAI for direct LLM calls (Token savings)
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

logger = logging.getLogger("ReportGenerator")

class ReportGenerator:
    """
    Generates a deterministic Markdown report by aggregating artifacts
    and using an LLM to write the narrative sections.
    """
    
    @staticmethod
    def generate(run_id: str, research_goal: str) -> str:
        """
        Compiles experiments/{run_id} into FINAL_REPORT.md
        """
        logger.info(f"Generating Final Report for Run {run_id}...")
        
        # 1. Setup Paths
        base_dir = Path(__file__).parent.parent / "experiments" / run_id
        report_path = base_dir / "FINAL_REPORT.md"
        
        if not base_dir.exists():
            return f"Error: Run directory {base_dir} does not exist."

        # 2. Load Artifacts
        dataset_meta = ReportGenerator._load_json(base_dir / "dataset_used.json")
        raw_results = ReportGenerator._load_json(base_dir / "raw_results.json")
        
        # Determine Dataset Info (Handle Procedural vs Loaded)
        ds_source = "Unknown"
        ds_type = "Unknown"
        ds_details = "{}"
        
        if dataset_meta.get("data_modality") == "procedural":
            ds_source = "Procedural Generation (Synthetic)"
            ds_type = "Synthetic Regression/Classification"
            ds_details = json.dumps(dataset_meta, indent=2)
        else:
            ds_source = dataset_meta.get('dataset_id') or dataset_meta.get('source_family', 'Unknown')
            ds_type = dataset_meta.get('type') or dataset_meta.get('requirements', {}).get('task_type', 'Unknown')
            # Prefer 'Details' if populated by dataset_resolver, else 'parameters'
            details_content = dataset_meta.get('Details') or dataset_meta.get('parameters', {})
            ds_details = json.dumps(details_content, indent=2)

        # 3. Visualizations
        plots = list(base_dir.glob("*.png"))
        plot_markdown = ""
        if plots:
            plot_markdown = "## 3.1 Visualizations\n\n"
            for p in plots:
                # Use relative path for Markdown
                rel_path = p.name
                plot_markdown += f"![{p.stem}]({rel_path})\n*Figure: {p.stem}*\n\n"
        
        # 4. Generate Narrative (LLM)
        narrative = ReportGenerator._generate_narrative(research_goal, dataset_meta, raw_results)
        
        # 5. Assemble Report (Deterministic Layout)
        # NOTE: Resource metrics are in Appendix, separate from scientific conclusions
        report_content = f"""# Final Research Report
**Run ID:** `{run_id}`
**Date:** {os.getenv("DATE", "N/A")}

---

## 1. Executive Summary
{narrative.get('executive_summary', 'Summary not available.')}

## 2. Methodology
**Research Goal:** {research_goal}

### Dataset
- **Source:** {ds_source}
- **Type:** {ds_type}
- **Details:** 
```json
{ds_details}
```

{narrative.get('methodology_description', '')}

## 3. Results & Discussion
{narrative.get('results_discussion', 'See raw results below.')}

{plot_markdown}

---

## Appendix A: Artifacts & Audit Trail

### A.1 Data Artifacts
- **Raw Results:** [raw_results.json](raw_results.json)
- **Dataset Metadata:** [dataset_used.json](dataset_used.json)

### A.2 Code & Logs
- **Experiment Code:** [run_experiment.py](run_experiment.py)
- **Execution Log:** [execution.log](execution.log)

### A.3 Resource Usage
> Note: These metrics are for auditing purposes only and do not affect scientific conclusions.

| Metric | Value |
|--------|-------|
| Report Generated | {os.getenv("DATE", "N/A")} |

---
*Generated by Epsilon Autonomous Research Engine*
*Narrative sections are grounded in artifact data only. No external information was used.*
"""
        
        # 6. Save
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report_content)
            
        logger.info(f"Report saved to {report_path}")
        
        # 7. Generate Standalone HTML
        try:
            html_path = ReportGenerator._generate_html(run_id, report_content, base_dir)
            logger.info(f"HTML Report saved to {html_path}")
        except Exception as e:
            logger.error(f"Failed to generate HTML report: {e}")
            html_path = None

        return str(report_path)

    @staticmethod
    def _generate_html(run_id: str, markdown_text: str, base_dir: Path) -> str:
        """
        Creates a standalone HTML file with professional 'Academic Lab' styling.
        """
        import base64
        import re
        
        html_path = base_dir / "FINAL_REPORT.html"
        
        # Image Replacer (Logic kept in case needed, but graphs are removed from MD upstream)
        def image_replacer(match):
            alt_text = match.group(1)
            img_filename = match.group(2)
            img_path = base_dir / img_filename
            if img_path.exists():
                with open(img_path, "rb") as img_f:
                    b64_data = base64.b64encode(img_f.read()).decode("utf-8")
                    return f'<div class="figure"><img src="data:image/png;base64,{b64_data}" alt="{alt_text}"><p class="caption">Figure: {alt_text}</p></div>'
            return f""

        # Regex
        html_content = re.sub(r'!\[(.*?)\]\((.*?)\)', image_replacer, markdown_text)
        html_content = re.sub(r'(?<!!)\[(.*?)\]\((.*?)\)', r'<a href="\2" target="_blank" class="artifact-link">\1</a>', html_content)
        
        # Structural Formatting
        # Fix: Handle title at start of string (no leading newline)
        html_content = re.sub(r'(^|\n)# (.*?)(\n|$)', r'\1<h1 class="title">\2</h1>\3', html_content)
        html_content = re.sub(r'\n## (.*?)\n', r'\n<h2 class="section-header">\1</h2>\n', html_content)
        html_content = re.sub(r'\n### (.*?)\n', r'\n<h3 class="subsection-header">\1</h3>\n', html_content)
        
        # Fix: Properly handle bold (**text**) and italic (*text*) pairs
        html_content = re.sub(r'\*\*(.*?)\*\*', r'<strong>\1</strong>', html_content)
        html_content = re.sub(r'(?<!\*)\*(?!\*)(.*?)(?<!\*)\*(?!\*)', r'<em>\1</em>', html_content)
        
        html_content = html_content.replace("```json", "<div class='code-block'><pre><code class='json'>").replace("```", "</code></pre></div>")
        
        # Turn bullet points into styled list items
        html_content = re.sub(r'\n- (.*)', r'\n<div class="list-item">• \1</div>', html_content)
        
        # 3.4 Fix newline flattening (Semantic Paragraphs)
        html_content = re.sub(r'\n{2,}', '</p><p>', html_content)
        html_content = f"<p>{html_content}</p>"

        # 3.1 Introduce semantic blocks
        html_content = html_content.replace(
            '<h2 class="section-header">1. Executive Summary</h2>',
            '<div class="section highlight"><h2 class="section-header">1. Executive Summary</h2>'
        )
        html_content = html_content.replace(
            '<h2 class="section-header">2. Methodology</h2>',
            '</div><div class="section"><h2 class="section-header">2. Methodology</h2>'
        )
        html_content = html_content.replace(
            '<h2 class="section-header">4. Audit Trail</h2>',
            '</div><div class="section archive"><h2 class="section-header">4. Audit Trail</h2>'
        )
        
        # Academic / Professional CSS
        full_html = f"""
        <!DOCTYPE html>
        <html lang="en">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>EPSILON LAB REPORT - {run_id}</title>
            <style>
                :root {{
                    --bg-color: #f9f9f9;
                    --card-bg: #ffffff;
                    --text-color: #1a1a1a;
                    --text-muted: #666666;
                    --accent-color: #2c3e50;  /* Dark Slate */
                    --accent-secondary: #2980b9; /* Professional Blue */
                    --border-color: #eaeaea;
                    --font-serif: 'Georgia', 'Times New Roman', serif;
                    --font-sans: 'Inter', 'Helvetica Neue', 'Arial', sans-serif;
                    --font-mono: 'Menlo', 'Monaco', 'Courier New', monospace;
                }}
                
                body {{
                    font-family: var(--font-sans);
                    background-color: var(--bg-color);
                    color: var(--text-color);
                    margin: 0;
                    padding: 40px;
                    line-height: 1.6;
                }}
                
                .container {{
                    max-width: 850px;
                    margin: 0 auto;
                    border: 1px solid #ddd;
                    padding: 60px;
                    background: var(--card-bg);
                    box-shadow: 0 4px 6px rgba(0,0,0,0.05);
                }}
                
                .report-header {{
                    text-align: center;
                    border-bottom: 2px solid var(--text-color);
                    padding-bottom: 20px;
                    margin-bottom: 40px;
                }}

                .title {{
                    font-family: var(--font-serif);
                    font-size: 2.2rem;
                    color: var(--text-color);
                    margin: 0;
                    font-weight: 700;
                }}
                
                .run-id {{
                    display: block;
                    margin-top: 10px;
                    font-size: 0.85rem;
                    color: var(--text-muted);
                    font-family: var(--font-mono);
                }}

                h2, .section-header {{
                    color: var(--accent-color);
                    font-family: var(--font-serif);
                    font-size: 1.5rem;
                    border-bottom: 1px solid #eee;
                    padding-bottom: 10px;
                    margin-top: 40px;
                    margin-bottom: 20px;
                    font-weight: 600;
                }}
                
                h3, .subsection-header {{
                    color: var(--accent-secondary);
                    font-size: 1.1rem;
                    margin-top: 25px;
                    font-weight: 600;
                    text-transform: uppercase;
                    letter-spacing: 0.5px;
                }}

                p, li {{
                    font-size: 1rem;
                    color: #333;
                    text-align: justify;
                }}

                .section.highlight {{
                    background: #f8fcfd;
                    border-left: 4px solid var(--accent-secondary);
                    padding: 20px 30px;
                    border-radius: 4px;
                }}

                .section.archive {{
                    background: #f5f5f5;
                    border: 1px dashed #ccc;
                    padding: 20px;
                    margin-top: 50px;
                }}

                .code-block {{
                    background: #f4f4f4;
                    border: 1px solid #ddd;
                    padding: 15px;
                    border-radius: 4px;
                    margin: 20px 0;
                    font-size: 0.85rem;
                    overflow-x: auto;
                    font-family: var(--font-mono);
                }}
                
                /* Styled Artifact Links */
                .artifact-link {{
                    color: var(--accent-secondary);
                    text-decoration: none;
                    border-bottom: 1px dotted var(--accent-secondary);
                    font-weight: 500;
                    transition: all 0.2s;
                }}
                
                .artifact-link:hover {{
                    background: #eef6fa;
                    color: #1a5276;
                }}

                .figure {{
                    border: 1px solid #eee;
                    background: #fff;
                    padding: 10px;
                    text-align: center;
                    margin: 30px 0;
                    border-radius: 4px;
                }}
                
                .caption {{
                    color: var(--text-muted);
                    font-size: 0.9rem;
                    margin-top: 10px;
                    font-style: italic;
                    font-family: var(--font-serif);
                }}
                
                .list-item {{
                    margin-bottom: 8px;
                    padding-left: 10px;
                }}

                img {{
                    max-width: 100%;
                    border: 1px solid #eee;
                }}
                
                .footer {{
                    margin-top: 80px;
                    padding-top: 20px;
                    border-top: 1px solid #eee;
                    font-size: 0.8rem;
                    color: var(--text-muted);
                    text-align: center;
                    font-family: var(--font-serif);
                }}

            </style>
        </head>
        <body>
            <div class="container">
                <div class="report-header">
                    <h1 class="title">Research Laboratory Report</h1>
                    <span class="run-id">RUN_ID: {run_id}</span>
                </div>
                <div class="content">
                    {html_content}
                    <div class="footer">
                        EPSILON AUTONOMOUS RESEARCH ENGINE<br>
                        <em>Generated Report • Verifiable Audit Trail</em>
                    </div>
                </div>
            </div>
        </body>
        </html>
        """
        
        with open(html_path, "w", encoding="utf-8") as f:
            f.write(full_html)
            
        return str(html_path)

    @staticmethod
    def _load_json(path: Path) -> Dict:
        if path.exists():
            try:
                return json.loads(path.read_text(encoding="utf-8"))
            except:
                return {"error": "Invalid JSON"}
        return {}

    @staticmethod
    def _validate_results(results: Dict) -> Dict:
        """
        Validates raw_results.json and attempts to extract Success Spec metrics.
        """
        validation = {
            "has_results": bool(results),
            "intervention_improved": None,
            "baseline_mean": None,
            "intervention_mean": None,
            "improvement_pct": None,
            "warning": None,
            "metric_name": "Unknown",
            "p_value": None
        }
        
        if not results:
            validation["warning"] = "No raw results available."
            return validation
            
        # Try to detect Success Spec from results if present (best effort)
        # Assuming run_eval.py output style
        if "rmse_ratio_mean" in results:
            validation["metric_name"] = "RMSE Ratio"
            validation["intervention_mean"] = results.get("rmse_improved_mean", 0)
            validation["baseline_mean"] = results.get("rmse_baseline_mean", 0)
            validation["p_value"] = results.get("wilcoxon_one_sided_p")
            
            ratio = results.get("rmse_ratio_mean", 1.0)
            threshold = results.get("rmse_ratio_threshold", 0.9)
            
            if ratio < threshold:
                validation["intervention_improved"] = True
                validation["improvement_pct"] = (1 - ratio) * 100
            else:
                validation["intervention_improved"] = False
                validation["improvement_pct"] = (1 - ratio) * 100 # likely negative
                
        # Existing logic for test_accuracy etc
        elif "test_accuracy" in results:
             test_acc = results.get("test_accuracy", {})
             if test_acc:
                baseline_key = None
                intervention_key = None
                for key in test_acc.keys():
                    lower_key = key.lower()
                    if "baseline" in lower_key or "control" in lower_key:
                        baseline_key = key
                    else:
                        intervention_key = key
                if baseline_key and intervention_key:
                    import numpy as np
                    baseline_vals = test_acc[baseline_key]
                    intervention_vals = test_acc[intervention_key]
                    if baseline_vals and intervention_vals:
                        validation["baseline_mean"] = float(np.mean(baseline_vals))
                        validation["intervention_mean"] = float(np.mean(intervention_vals))
                        if validation["intervention_mean"] > validation["baseline_mean"]:
                            validation["intervention_improved"] = True
                            validation["improvement_pct"] = ((validation["intervention_mean"] - validation["baseline_mean"]) / validation["baseline_mean"]) * 100
                        else:
                            validation["intervention_improved"] = False
                            validation["improvement_pct"] = ((validation["intervention_mean"] - validation["baseline_mean"]) / validation["baseline_mean"]) * 100
                            validation["warning"] = f"Intervention performed worse."

        return validation
    
    @staticmethod
    def _generate_narrative(goal: str, dataset: Dict, results: Dict) -> Dict:
        """
        Uses direct LLM call to write sections.
        """
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key or not OpenAI:
            return {
                "executive_summary": "LLM generation unavailable (Missing Key or Lib).",
                "methodology_description": "See structured data above.",
                "results_discussion": "See qualitative metrics below."
            }
        
        # CRITICAL: Validate results BEFORE passing to LLM
        validation = ReportGenerator._validate_results(results)
        
        # Build validation context for LLM
        validation_context = ""
        if validation["warning"]:
            validation_context = f"""
            ⚠️ CRITICAL VALIDATION WARNING: {validation["warning"]}
            You MUST report this negative result honestly. Do NOT claim improvement where none exists.
            """
        elif validation["intervention_improved"]:
            validation_context = f"""
            ✅ Validated Improvement: {validation["improvement_pct"]:.2f}%
            P-Value: {validation["p_value"] if validation["p_value"] else 'N/A'}
            """
        else:
            validation_context = "No validation data available. Report only what can be verified from results."
            
        try:
            client = OpenAI(api_key=api_key)
            
            prompt = f"""
            You are writing a Technical Research Log for an autonomous AI engine.
            TONE: Professional, academic lab report, concise, objective. No "cyberpunk" slang.
            
            GOAL: {goal}
            DATASET: {json.dumps(dataset)}
            RESULTS: {str(results)[:2000]}
            
            {validation_context}
            
            ═══════════════════════════════════════════════════════════════════════
            ██ ANTI-HALLUCINATION PROTOCOL ██
            ═══════════════════════════════════════════════════════════════════════
            
            You are STRICTLY FORBIDDEN from:
            1. Inventing numbers not present in RESULTS
            2. Claiming improvements not verified in validation_context
            3. Speculating about causes without data evidence
            
            You MUST:
            1. ONLY report values that appear in the RESULTS data above
            2. Quote specific numbers when discussing metrics
            3. State "Data not available" if information is missing
            4. If intervention failed, say so directly without hedging
            
            Task: Write 3 sections in JSON format.
            1. "executive_summary": High-level technical abstract (approx 100 words). 
               - MUST cite specific metric values from the data
               - State explicitly whether the goal was achieved
            2. "methodology_description": Precise description of the approach (approx 50 words).
               - Reference only what is in DATASET
            3. "results_discussion": Analytical interpretation (approx 150 words).
               - Every claim must reference a specific value from RESULTS
               - If p-value is present, discuss statistical significance
               - Briefly mention symmetry/skewness check if present in results (justify it: "suggests unbiased generation")
            
            Output JSON ONLY.
            """
            
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"}
            )
            
            content = response.choices[0].message.content
            return json.loads(content)
            
        except Exception as e:
            logger.error(f"LLM Narrative Generation Failed: {e}")
            return {
                "executive_summary": f"Generation Error: {str(e)}",
                "methodology_description": "Error",
                "results_discussion": "Error"
            }
