# Epsilon: Autonomous Research Engine
## Architecture & System Overview

---

## ðŸ“‹ Executive Summary

**Epsilon** is an autonomous research engine that conducts end-to-end scientific discovery without human intervention. Given a high-level research question, it:

- ðŸ” **Researches** background context from the web
- ðŸ§ª **Designs** statistically rigorous experiments
- ðŸ’» **Generates & Executes** Python code
- ðŸ“Š **Evaluates** results using statistical validation
- ðŸ“ **Reports** findings in a professional HTML/Markdown report

---

## ðŸ—ï¸ High-Level Architecture

```mermaid
flowchart TB
    subgraph User["ðŸ‘¤ User Interface"]
        RQ["Research Question"]
    end

    subgraph Controller["ðŸŽ›ï¸ Research Controller"]
        ORCH["Orchestrator"]
    end

    subgraph Agents["ðŸ¤– Agent System"]
        RA["Research Agent"]
        EA["Experiment Design Agent"]
        XA["Execution Agent"]
        VA["Evaluation Agent"]
    end

    subgraph Memory["ðŸ§  Memory System"]
        EM["Evidence Memory"]
        KM["Knowledge Memory"]
        RM["Run Memory"]
    end

    subgraph Output["ðŸ“Š Outputs"]
        CODE["Generated Python Code"]
        DATA["Raw Results (JSON/CSV)"]
        REPORT["Final Report (HTML/MD)"]
        VIZ["Visualizations (PNG)"]
    end

    RQ --> ORCH
    ORCH --> RA
    RA --> EA
    EA --> XA
    XA --> VA
    VA -->|Feedback Loop| EA

    RA <--> EM
    EA <--> KM
    EA <--> RM
    VA --> RM

    XA --> CODE
    XA --> DATA
    XA --> VIZ
    ORCH --> REPORT
```

---

## ðŸ”„ The Epistemic Loop

The core innovation of Epsilon is its **self-correcting research loop**. The system iterates until it achieves statistically valid results.

```mermaid
flowchart LR
    subgraph Phase1["Phase 1: Research"]
        R1["Web Search"]
        R2["PDF Reading"]
        R3["Evidence Extraction"]
    end

    subgraph Phase2["Phase 2: Design"]
        D1["Hypothesis Formulation"]
        D2["Variable Definition"]
        D3["Statistical Plan"]
    end

    subgraph Phase3["Phase 3: Execution"]
        E1["Dataset Resolution"]
        E2["Code Generation"]
        E3["Script Execution"]
    end

    subgraph Phase4["Phase 4: Evaluation"]
        V1["Assumption Checks"]
        V2["Statistical Tests"]
        V3["Verdict Decision"]
    end

    Phase1 --> Phase2 --> Phase3 --> Phase4
    Phase4 -->|"âŒ Failed"| Phase2
    Phase4 -->|"âœ… Success"| REPORT["ðŸ“ Final Report"]
```

---

## ðŸ¤– Agent Breakdown

### 1. Research Agent (`research_agent.py`)

**Role**: The Literature Reviewer

| Tool | Purpose |
|------|---------|
| `web_search` | Search the web using Tavily API |
| `read_pdf` | Extract text from academic PDFs |
| `read_webpage` | Parse HTML content from articles |
| `save_evidence` | Persist key findings to Evidence Memory |
| `query_evidence` | Check existing evidence to avoid duplication |

**Output**: Structured research notes with citations and claims

---

### 2. Experiment Design Agent (`experiment_agent.py`)

**Role**: The Scientist

| Tool | Purpose |
|------|---------|
| `query_knowledge` | Retrieve validated facts from past runs |
| `query_past_failures` | Avoid repeating previous mistakes |
| `corpus_query` | Search the evidence corpus |

**Output**: JSON Experiment Specification including:
- Research question
- Hypotheses (H0, H1)
- Variables (independent, dependent, controls)
- Model design
- Dataset requirements
- Statistical analysis plan

---

### 3. Execution Agent (`execution_agent.py`)

**Role**: The Lab Technician

| Tool | Purpose |
|------|---------|
| `dataset_resolver` | Resolve data sources (sklearn, procedural, external) |
| `execute_experiment` | Run the generated Python script |
| `install_package` | Dynamically install missing dependencies |

**Output**: 
- `run_experiment.py` â€“ Generated experiment code
- `raw_results.json` â€“ Experiment data
- `*.png` â€“ Visualizations

---

### 4. Evaluation Agent (`evaluation_agent.py`)

**Role**: The Statistician (Isolated by Design)

| Tool | Purpose |
|------|---------|
| `run_statistical_test` | Execute T-tests, ANOVA, Mann-Whitney, etc. |
| `verify_assumptions` | Check normality and other assumptions |
| `python_analysis_tool` | Execute custom Python analysis code |

**Output**: Evaluation verdict with:
- Hypothesis decision (Reject H0 / Fail to Reject)
- P-value and statistical summary
- Outcome classification
- Feedback for iteration

> **ðŸ”’ Epistemic Integrity**: The Evaluation Agent is intentionally isolated from the Design Agent to prevent p-hacking and preserve scientific rigor.

---

## ðŸ§  Memory Architecture

Epsilon uses a **three-tier memory system** backed by Supabase:

```mermaid
flowchart TB
    subgraph MemoryService["Memory Service (Facade)"]
        MS["MemoryService"]
    end

    subgraph Backends["Memory Backends"]
        EM["Evidence Memory<br/>ðŸ“„ Raw research findings"]
        KM["Knowledge Memory<br/>ðŸ§  Validated facts (crystallized)"]
        RM["Run Memory<br/>ðŸ“‹ Audit logs of all iterations"]
    end

    subgraph Storage["Supabase Storage"]
        T1["research_evidence"]
        T2["knowledge_memory"]
        T3["run_memory"]
    end

    MS --> EM
    MS --> KM
    MS --> RM

    EM --> T1
    KM --> T2
    RM --> T3
```

### Memory Types Explained

| Memory Type | Purpose | Persistence | Access Pattern |
|-------------|---------|-------------|----------------|
| **Evidence Memory** | Stores raw claims, PDFs, web sources | Per-run | Read-before-write deduplication |
| **Knowledge Memory** | Stores validated scientific conclusions | Crystallized on success | Append-only, high-precision |
| **Run Memory** | Stores complete audit trail of all iterations | Per-iteration | Query by failure type |

---

## ðŸ“Š Output Generation

### Report Generator (`core/report_generator.py`)

Produces a comprehensive scientific report:

```mermaid
flowchart LR
    subgraph Artifacts["Experiment Artifacts"]
        A1["raw_results.json"]
        A2["experiment_design.json"]
        A3["*.png visualizations"]
    end

    subgraph LLM["LLM Generation"]
        L1["Executive Summary"]
        L2["Methodology Section"]
        L3["Results Discussion"]
    end

    subgraph Output["Final Report"]
        O1["FINAL_REPORT.md"]
        O2["FINAL_REPORT.html"]
    end

    Artifacts --> LLM --> Output
```

**Report Sections**:
1. **Executive Summary** â€“ LLM-generated overview
2. **Research Context** â€“ Goal and hypothesis
3. **Audit Trail** â€“ Complete iteration history
4. **Methodology** â€“ Dataset and experimental design
5. **Results & Discussion** â€“ Statistical findings with embedded visualizations

---

## ðŸ“ Directory Structure

```
Epsilon/
â”œâ”€â”€ controller.py          # ðŸŽ›ï¸ Central orchestrator
â”œâ”€â”€ research_agent.py      # ðŸ” Web research & evidence
â”œâ”€â”€ experiment_agent.py    # ðŸ§ª Experiment design
â”œâ”€â”€ execution_agent.py     # ðŸ’» Code generation & execution
â”œâ”€â”€ evaluation_agent.py    # ðŸ“Š Statistical validation
â”œâ”€â”€ main.py                # ðŸš€ Entry point
â”‚
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ identity.py        # User identity management
â”‚   â””â”€â”€ report_generator.py # Report generation
â”‚
â”œâ”€â”€ memory/
â”‚   â”œâ”€â”€ memory_service.py   # Facade for all memory ops
â”‚   â”œâ”€â”€ evidence_memory.py  # Raw research findings
â”‚   â”œâ”€â”€ knowledge_memory.py # Crystallized facts
â”‚   â”œâ”€â”€ run_memory.py       # Audit logs
â”‚   â”œâ”€â”€ supabase_client.py  # Database connection
â”‚   â”œâ”€â”€ policies.py         # Crystallization rules
â”‚   â””â”€â”€ types.py            # Shared type definitions
â”‚
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ {run_id}/           # Per-run isolated directory
â”‚       â”œâ”€â”€ run_experiment.py
â”‚       â”œâ”€â”€ raw_results.json
â”‚       â”œâ”€â”€ comparison_plot.png
â”‚       â””â”€â”€ FINAL_REPORT.html
â”‚
â””â”€â”€ requirements.txt        # Dependencies
```

---

## ðŸ”‘ Key Design Principles

| Principle | Implementation |
|-----------|----------------|
| **Epistemic Integrity** | Evaluation Agent isolated from Design Agent |
| **Self-Correction** | Feedback loop with up to N iterations |
| **Run Isolation** | Each run gets its own `experiments/{run_id}/` directory |
| **Memory Persistence** | All findings stored in Supabase for cross-run learning |
| **Auditability** | Complete audit trail in Run Memory |
| **Reproducibility** | All code and data artifacts are preserved |

---

## ðŸš€ System Flow Summary

```mermaid
sequenceDiagram
    participant User
    participant Controller
    participant Research as Research Agent
    participant Design as Design Agent
    participant Execute as Execution Agent
    participant Eval as Evaluation Agent
    participant Memory as Memory System
    participant Report as Report Generator

    User->>Controller: Research Goal
    Controller->>Research: Gather Context
    Research->>Memory: Save Evidence
    Research-->>Controller: Research Notes

    loop Epistemic Loop (max N iterations)
        Controller->>Design: Create Experiment Spec
        Design->>Memory: Query Knowledge & Failures
        Design-->>Controller: Experiment Specification

        Controller->>Execute: Generate & Run Code
        Execute-->>Controller: Results + Artifacts

        Controller->>Eval: Validate Results
        Eval->>Memory: Record Iteration
        
        alt Success
            Eval-->>Controller: âœ… Hypothesis Validated
            Memory->>Memory: Crystallize Knowledge
        else Failure
            Eval-->>Controller: âŒ Feedback for Revision
        end
    end

    Controller->>Report: Generate Final Report
    Report-->>User: FINAL_REPORT.html
```

---

## âœ… What's Working

| Component | Status | Description |
|-----------|--------|-------------|
| Research Agent | âœ… Operational | Web search, PDF parsing, evidence extraction |
| Experiment Design | âœ… Operational | JSON spec generation with hypothesis |
| Code Execution | âœ… Operational | Script generation, dataset resolution, execution |
| Statistical Evaluation | âœ… Operational | T-tests, ANOVA, assumption checks |
| Memory System | âœ… Operational | Evidence, Knowledge, Run Memory with Supabase |
| Self-Correction Loop | âœ… Operational | Iterates on failures with feedback |
| Report Generation | âœ… Operational | HTML/Markdown with embedded plots |
| Run Isolation | âœ… Operational | Per-run experiment directories |

---

## ðŸ“Œ Summary

Epsilon represents a complete autonomous research pipeline that:

1. **Autonomously researches** a topic using web and PDF sources
2. **Designs experiments** with proper statistical rigor
3. **Generates and executes** Python code in isolation
4. **Validates results** through independent statistical analysis
5. **Learns from failures** via a self-correcting feedback loop
6. **Produces professional reports** with visualizations and audit trails

The system is designed with **epistemic integrity** at its core, ensuring that the scientist (Design Agent) and the statistician (Evaluation Agent) remain separated to prevent bias in the research process.

---

*Generated for Epsilon Project Presentation*
