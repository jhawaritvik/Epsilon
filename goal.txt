Autonomous Research System ‚Äì Agent Flow & Structure

## üéØ Goal of the System

Conduct **end-to-end research** on a topic:

- explore literature & ideas
- form hypotheses
- implement experiments (via Jupyter)
- analyze results
- iterate until quality threshold is met

---

## üß† Core Agents (WHO does WHAT)

### 1Ô∏è‚É£ Research Exploration Agent

**Role**

- **Build a high-quality research corpus** for future experiment execution
- **Targeted Information Gatherer**: Filter and search specifically for highly relevant papers and reports
- **Deep Dive & Filtered Ingest**: Download core PDFs and capture essential technical data (formulas, benchmarks) while excluding noise
- **No Inferencing**: Dedicated to building a lean, relevant information base without design or action responsibilities.

**Inputs**

- Research question / domain
- Previous iteration context (if any)

**Outputs**

- **Research Corpus Index**: A structured catalog of all ingested papers, notes, and technical snippets
- **Topic Summary**: A descriptive map of collected information and its location in memory
- **Raw Technical Data**: High-fidelity records available for downstream reasoning agents

**Tools**

- Web search (Tavily)
- PDF reader (download & extract text)
- Notes memory (save/search)

---

### 2Ô∏è‚É£ Experiment Design Agent

**Role**

- **Research-to-Experiment Translation**: Convert the Research Corpus into a concrete, executable experiment design.
- **Hypothesis & Variable Definition**: Formulate testable hypotheses (H‚ÇÄ, H‚ÇÅ), identify independent/dependent variables, and define controls.
- **Experimental & Statistical Specification**: Specify architectures, datasets, baselines, metrics, and the statistical analysis plan required for valid evaluation.
- **Strict Structured Output**: Produce a single valid JSON object with detailed specifications.

**Inputs**

- **User Research Goal** (Original prompt/intent).
- **Research Notes/Corpus** (List of strings/excerpts from the Research Agent).

**Outputs**

- **JSON Object** containing:
  - **Experiment Specification**:
    - Research question & Hypotheses (H‚ÇÄ, H‚ÇÅ)
    - Variables (independent, dependent, controls)
    - Model design & Dataset requirements
  - **Statistical Analysis Plan**:
    - Primary/Fallback tests, Alpha, Assumptions
  - **Power Analysis Plan**:
    - Method, Effect size, Target power
  - **Evidence Used**: References to the research notes used to justify decisions.

**Tools**

- **Corpus Query Tool**: Evidence-grounded retrieval over research notes (using TF-IDF) to justify experimental design choices.
- *(Note: Experimental logic, statistical selection, and power analysis specifications are handled via internal chain-of-thought/LLM reasoning, not external tools)*

---

### 3Ô∏è‚É£ Code & Execution Agent

#### Purpose
The **Code & Execution Agent** is responsible for the **deterministic realization** of an experiment defined by the **Experiment Design Agent**. It focuses strictly on **execution and data resolution**, making no scientific, statistical, or interpretive decisions.

---

#### Role

##### 1. Deterministic Implementation
- **Authoritative Translation**: Convert the **Experiment Specification (JSON)** into executable artifacts without deviation.
- **Specification Loyalty**: Treat the specification as authoritative and immutable; implement *exactly* what is defined.

##### 2. Dataset Resolution (Execution-Level)
- **Constraint Satisfaction**: Resolve dataset requirements into a concrete instance. Supports **HuggingFace Hub search** and **Procedural Sampling** (for PINNs).
- **Validation**: Select a dataset *only* if it strictly satisfies all scientific constraints (sample size, task type, etc.).
- **Traceability**: Log resolved dataset metadata (`dataset_used.json`) for reproducibility.

##### 3. Autonomous Execution & Self-Correction
- **Sandboxed Runtime**: Execute all experiments inside an isolated `experiments/` directory to protect the core repository.
- **Iterative Debugging**: Use an **Autonomic Self-Correction Loop** (up to 5 retries) to fix implementation bugs (imports, shape mismatches, autograd nuances) by analyzing stack traces.

##### 4. Evidence Generation (Raw Data)
- **Artifact Production**: Persist raw experimental evidence (metrics, logs) in structured, machine-readable formats.
- **Data Integrity**: Ensure all outputs are reproducible, timestamped, and traceable.

---

#### Inputs

- **Experiment Specification (JSON)**: The single source of truth for research questions, hypotheses, variables, model design, and dataset requirements.
- **Statistical Analysis Plan**: Grounding for identifying what specific metrics and granularity must be logged.
- **Revision Directives (JSON)**: Optional instructions from the Evaluation Agent for additional runs or logging adjustments.

---

#### Outputs

- **Primary Artifacts** (Saved in `experiments/`):
  - `run_experiment.py`: The executable experiment logic.
  - `dataset_used.json`: Resolution record (name, version, sample counts, verification).
  - `raw_results.{json|csv}`: Per-run, per-seed metrics ONLY (no aggregation).
  - `execution.log`: Full runtime telemetry and error traces.
- **Secondary Artifacts**:
  - **Descriptive Visualizations**: Loss curves and metric trajectories (strictly descriptive).
  - **Model Checkpoints**: Saved *only* if explicitly specified in the design.

---

#### Capabilities & Tools

- **Tool: `dataset_resolver`**: Searches HuggingFace Hub or flags for synthetic data generation based on Scientist requirements.
- **Tool: `execute_experiment`**: Orchestrates the sandboxed execution environment and captures all runtime telemetry.
- **Machine Learning Stack**: Deep integration with `torch` (explicit seed control), `numpy`, and `datasets`.

---

#### Internal Process (The Loop)
1. **Resolve**: Call `dataset_resolver` to identify the data source (HF vs. Internal).
2. **Translate**: Convert the JSON Experiment Specification into a standalone `run_experiment.py` script.
3. **Execute**: Run the script inside the sandboxed `experiments/` directory.
4. **Debug**: If failure occurs, inspect `execution.log`, diagnose the root cause (e.g. autograd leaf nuances), and fix the code.
5. **Finalize**: Produce the final results bundle (`raw_results.json`, `execution.log`, `dataset_used.json`).

---

#### Operational Constraints (LOCKED)

- ‚ùó **No Design Authority**: Never modify hypotheses, variables, or scientific requirements.
- ‚ùó **No Statistical Authority**: Never compute p-values, perform hypothesis tests, or aggregate results.
- ‚ùó **No Interpretation**: Never judge experiment quality or suggest improvements.
- ‚ùó **Halt on Retries**: Stop and report to the user if the bug persists after 5 self-correction attempts.

---

#### Mental Model: The Lab Technician
- **Experiment Design Agent** ‚Üí Scientist (Reasoning)
- **Code & Execution Agent** ‚Üí Lab Technician (Deterministic Action)
- **Evaluation Agent** ‚Üí Reviewer (Validation)

---
### 4Ô∏è‚É£ Evaluation / Analysis Agent

#### Role
- **Statistical Executor**: Execute only the statistical tests and assumption checks specified in the Analysis Protocol.
- **Scientific Validator**: Evaluate results strictly against predefined hypotheses, thresholds, and criteria.
- **Decision Gatekeeper**: Produce a formal accept/reject/iterate decision based on statistical evidence.

> **Execution Boundary**: This agent calculates and judges within a fixed protocol. It does not redesign experiments or introduce new criteria.

---

#### Inputs
- **Analysis Protocol (Authoritative)**:
    - Statistical tests to run
    - Significance level (Œ±)
    - Assumptions to verify
    - Decision thresholds
- **Evaluation Artifacts**:
    - Raw metrics (`raw_results.json`)
    - Logs and plots
- **Research Context**:
    - Research question
    - $H_0$ / $H_1$ (from Design Agent)

---

#### Outputs
- **Statistical Results**:
    - Calculated p-values, confidence intervals, and effect sizes.
- **Assumption Validation**:
    - Normality, independence, and variance checks.
    - Explicit pass/fail per assumption.
- **Hypothesis Decision**: `Reject H‚ÇÄ` / `Fail to reject H‚ÇÄ`.
- **Outcome Classification**: `robust` / `promising` / `spurious` / `failed` (based only on protocol-defined rules).
- **Next-Step Routing Recommendation**:
    - `iterate` ‚Üí return to Design Agent
    - `refine` ‚Üí return to Design Agent with flagged issues
    - `stop` ‚Üí experiment concluded
- **Rationale**: Traceable explanation referencing statistics and thresholds.

---

#### Capabilities & Tools
- **Statistical Computation**: Tests specified in protocol (e.g., t-test, ANOVA, Mann‚ÄìWhitney).
- **Assumption Checks**: As explicitly listed in protocol.
- **Comparison Memory**: Previous experiment outcomes and trend tracking across iterations.

---

#### Operational Constraints (LOCKED)
- ‚ùó **No Invention**: Must not invent new metrics or tests.
- ‚ùó **Immutable Parameters**: Must not change Œ± or thresholds.
- ‚ùó **No Reinterpretation**: Must not reinterpret $H_0$ / $H_1$.
- ‚ùó **No Design Authority**: Must not suggest architectural or dataset changes.
- ‚ùó **Protocol Loyalty**: Must not override the Analysis Protocol.

> [!IMPORTANT]
> **Escalation Policy**: If results are ambiguous or protocol is insufficient, **escalate to Design Agent**. Do not self-resolve ambiguity.

## üß≠ Control Layer (HOW things flow)

### Controller (Python / LangGraph)

**Responsibilities**

- Maintain global state
- Route outputs between agents
- Enforce iteration limits
- Prevent infinite loops

**State Example**

```json
{
"iteration":3,
"hypothesis":"...",
"quality_score":0.71,
"max_iterations":8,
"status":"iterate"
}
```

‚ùó This is **NOT an agent**

‚ùó This is **deterministic control**

---

## üîÅ High-Level Flow (SEQUENCE)

1. **Research Exploration Agent**: Build corpus and technical index.
2. **Experiment Design Agent**: Translate research into formal JSON specification.
3. **Code & Execution Agent**: Implement and execute experiments in Jupyter.
4. **Evaluation & Analysis Agent**: Statistically validate results and decide next steps.
5. **Controller**: Decide whether to iterate (loop to 2 or 3) or terminate.

---

## üß∞ Tool Summary (WHAT is connected)

### LLM Tools

- OpenAI Agent SDK (for agents)
- Structured outputs (JSON)

### Execution Tools

- Jupyter Notebook kernel
- Python execution
- GPU access

### Data Tools

- Dataset loaders
- File system
- Experiment logs

### Memory

- Research notes store
- Experiment history store
- Failure patterns store