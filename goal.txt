Autonomous Research System â€“ Agent Flow & Structure

## ğŸ¯ Goal of the System

Conduct **end-to-end research** on a topic:

- explore literature & ideas
- form hypotheses
- implement experiments (via Jupyter)
- analyze results
- iterate until quality threshold is met

---

## ğŸ§  Core Agents (WHO does WHAT)

### 1ï¸âƒ£ Research Exploration Agent

**Role**

- **Build a high-quality research corpus** for future experiment execution
- **Targeted Information Gatherer**: Filter and search specifically for highly relevant papers and reports
- **Deep Dive & Filtered Ingest**: Download core PDFs and capture essential technical data (formulas, benchmarks) while excluding noise
- **No Inferencing**: Dedicated to building a lean, relevant information base without design or action responsibilities.

**Inputs**

- Research question / domain
- Previous iteration context (if any)

**Outputs**

- **Research Corpus Index**: A structured catalog of all ingested papers, notes, and technical snippets
- **Topic Summary**: A descriptive map of collected information and its location in memory
- **Raw Technical Data**: High-fidelity records available for downstream reasoning agents

**Tools**

- Web search (Tavily)
- PDF reader (download & extract text)
- Notes memory (save/search)

---

### 2ï¸âƒ£ Experiment Design Agent

**Role**

- **Research-to-Experiment Translation**: Convert the Research Corpus into a concrete, executable experiment design.
- **Hypothesis & Variable Definition**: Formulate testable hypotheses (Hâ‚€, Hâ‚), identify independent/dependent variables, and define controls.
- **Experimental & Statistical Specification**: Specify architectures, datasets, baselines, metrics, and the statistical analysis plan required for valid evaluation.
- **Strict Structured Output**: Produce a single valid JSON object with detailed specifications.

**Inputs**

- **User Research Goal** (Original prompt/intent).
- **Research Notes/Corpus** (List of strings/excerpts from the Research Agent).

**Outputs**

- **JSON Object** containing:
  - **Experiment Specification**:
    - Research question & Hypotheses (Hâ‚€, Hâ‚)
    - Variables (independent, dependent, controls)
    - Model design & Dataset requirements
  - **Statistical Analysis Plan**:
    - Primary/Fallback tests, Alpha, Assumptions
  - **Power Analysis Plan**:
    - Method, Effect size, Target power
  - **Evidence Used**: References to the research notes used to justify decisions.

**Tools**

- **Corpus Query Tool**: Evidence-grounded retrieval over research notes (using TF-IDF) to justify experimental design choices.
- *(Note: Experimental logic, statistical selection, and power analysis specifications are handled via internal chain-of-thought/LLM reasoning, not external tools)*

---

### 3ï¸âƒ£ Code & Execution Agent

#### Purpose
The **Code & Execution Agent** is responsible for the **deterministic realization** of an experiment defined by the **Experiment Design Agent**. It focuses strictly on **execution and data resolution**, making no scientific, statistical, or interpretive decisions.

---

#### Role

##### 1. Deterministic Implementation
- **Authoritative Translation**: Convert the **Experiment Specification (JSON)** into executable artifacts without deviation.
- **Specification Loyalty**: Treat the specification as authoritative and immutable; implement *exactly* what is defined.

##### 2. Dataset Resolution (Execution-Level)
- **Constraint Satisfaction**: Resolve dataset requirements into a concrete instance. Supports **HuggingFace Hub search** and **Procedural Sampling** (for PINNs).
- **Validation**: Select a dataset *only* if it strictly satisfies all scientific constraints (sample size, task type, etc.).
- **Traceability**: Log resolved dataset metadata (`dataset_used.json`) for reproducibility.

##### 3. Autonomous Execution & Self-Correction
- **Sandboxed Runtime**: Execute all experiments inside an isolated `experiments/` directory to protect the core repository.
- **Iterative Debugging**: Use an **Autonomic Self-Correction Loop** (up to 5 retries) to fix implementation bugs (imports, shape mismatches, autograd nuances) by analyzing stack traces.

##### 4. Evidence Generation (Raw Data)
- **Artifact Production**: Persist raw experimental evidence (metrics, logs) in structured, machine-readable formats.
- **Data Integrity**: Ensure all outputs are reproducible, timestamped, and traceable.

---

#### Inputs

- **Experiment Specification (JSON)**: The single source of truth for research questions, hypotheses, variables, model design, and dataset requirements.
- **Statistical Analysis Plan**: Grounding for identifying what specific metrics and granularity must be logged.
- **Revision Directives (JSON)**: Optional instructions from the Evaluation Agent for additional runs or logging adjustments.

---

#### Outputs

- **Primary Artifacts** (Saved in `experiments/`):
  - `run_experiment.py`: The executable experiment logic.
  - `dataset_used.json`: Resolution record (name, version, sample counts, verification).
  - `raw_results.{json|csv}`: Per-run, per-seed metrics ONLY (no aggregation).
  - `execution.log`: Full runtime telemetry and error traces.
- **Secondary Artifacts**:
  - **Descriptive Visualizations**: Loss curves and metric trajectories (strictly descriptive).
  - **Model Checkpoints**: Saved *only* if explicitly specified in the design.

---

#### Capabilities & Tools

- **Tool: `dataset_resolver`**: Searches HuggingFace Hub or flags for synthetic data generation based on Scientist requirements.
- **Tool: `execute_experiment`**: Orchestrates the sandboxed execution environment and captures all runtime telemetry.
- **Machine Learning Stack**: Deep integration with `torch` (explicit seed control), `numpy`, and `datasets`.

---

#### Internal Process (The Loop)
1. **Resolve**: Call `dataset_resolver` to identify the data source (HF vs. Internal).
2. **Translate**: Convert the JSON Experiment Specification into a standalone `run_experiment.py` script.
3. **Execute**: Run the script inside the sandboxed `experiments/` directory.
4. **Debug**: If failure occurs, inspect `execution.log`, diagnose the root cause (e.g. autograd leaf nuances), and fix the code.
5. **Finalize**: Produce the final results bundle (`raw_results.json`, `execution.log`, `dataset_used.json`).

---

#### Operational Constraints (LOCKED)

- â— **No Design Authority**: Never modify hypotheses, variables, or scientific requirements.
- â— **No Statistical Authority**: Never compute p-values, perform hypothesis tests, or aggregate results.
- â— **No Interpretation**: Never judge experiment quality or suggest improvements.
- â— **Halt on Retries**: Stop and report to the user if the bug persists after 5 self-correction attempts.

---

#### Mental Model: The Lab Technician
- **Experiment Design Agent** â†’ Scientist (Reasoning)
- **Code & Execution Agent** â†’ Lab Technician (Deterministic Action)
- **Evaluation Agent** â†’ Reviewer (Validation)

---
### 4ï¸âƒ£ Evaluation / Analysis Agent

#### Role
- **Statistical Executor**: Execute only the statistical tests and assumption checks specified in the Analysis Protocol.
- **Scientific Validator**: Evaluate results strictly against predefined hypotheses, thresholds, and criteria.
- **Decision Gatekeeper**: Produce a formal accept/reject/iterate decision based on statistical evidence.

> **Execution Boundary**: This agent calculates and judges within a fixed protocol. It does not redesign experiments or introduce new criteria.

---

#### Inputs
- **Analysis Protocol (Authoritative)**:
    - Statistical tests to run
    - Significance level (Î±)
    - Assumptions to verify
    - Decision thresholds
- **Evaluation Artifacts**:
    - Raw metrics (`raw_results.json`)
    - Logs and plots
- **Research Context**:
    - Research question
    - $H_0$ / $H_1$ (from Design Agent)

---

#### Outputs
- **Statistical Results**:
    - Calculated p-values, confidence intervals, and effect sizes.
- **Assumption Validation**:
    - Normality, independence, and variance checks.
    - Explicit pass/fail per assumption.
- **Hypothesis Decision**: `Reject Hâ‚€` / `Fail to reject Hâ‚€`.
- **Outcome Classification**: `robust` / `promising` / `spurious` / `failed` (based only on protocol-defined rules).
- **Next-Step Routing Recommendation**:
    - `iterate` â†’ return to Design Agent
    - `refine` â†’ return to Design Agent with flagged issues
    - `stop` â†’ experiment concluded
- **Rationale**: Traceable explanation referencing statistics and thresholds.

---

#### Capabilities & Tools
- **Statistical Computation**: Tests specified in protocol (e.g., t-test, ANOVA, Mannâ€“Whitney).
- **Assumption Checks**: As explicitly listed in protocol.
- **Comparison Memory**: Previous experiment outcomes and trend tracking across iterations.

---

#### Operational Constraints (LOCKED)
- â— **No Invention**: Must not invent new metrics or tests.
- â— **Immutable Parameters**: Must not change Î± or thresholds.
- â— **No Reinterpretation**: Must not reinterpret $H_0$ / $H_1$.
- â— **No Design Authority**: Must not suggest architectural or dataset changes.
- â— **Protocol Loyalty**: Must not override the Analysis Protocol.

> [!IMPORTANT]
> **Escalation Policy**: If results are ambiguous or protocol is insufficient, **escalate to Design Agent**. Do not self-resolve ambiguity.





ğŸ§­ Controller / Control Layer â€” Reference Specification
Purpose (LOCKED)

The Controller is a deterministic orchestration layer responsible for:

Routing execution between agents

Maintaining global state

Enforcing stopping conditions

Preventing infinite loops

â— The controller is NOT an agent
â— The controller does NOT use an LLM
â— The controller does NOT reason or interpret

Core Principle (Non-Negotiable)

Agents emit signals.
The controller routes execution.

Agents never call each other.
Agents never decide the next step.

Controller Responsibilities
1ï¸âƒ£ Global State Management

The controller owns and updates a single authoritative state object.

Required State Fields
{
  "iteration": 0,
  "max_iterations": 8,
  "last_agent": "design",
  "verdict": null,
  "issue_type": null,
  "status": "running"
}

Notes

iteration increments only after Evaluation

verdict comes only from Evaluation Agent

issue_type comes only from Evaluation or Code Agent

status âˆˆ { running, iterate, stop }

2ï¸âƒ£ Deterministic Routing (Core Job)

Routing is a pure function:

next_agent = route(state)


No randomness.
No LLM calls.
No interpretation.

Routing Rules (Authoritative)
Hard Stop Conditions (Checked First)
IF verdict == "robust" â†’ STOP
IF iteration >= max_iterations â†’ STOP

Iteration Routing Table
Signal Source	Condition	Next Agent
Evaluation	issue_type == "execution"	Code
Evaluation	issue_type == "design"	Design
Evaluation	issue_type == "data"	Research
Evaluation	verdict âˆˆ {promising, spurious, failed}	Iterate
Code	execution_failed == true	Code
Design	spec_incomplete == true	Research
Default Fallback
IF no rule matches â†’ STOP


Fail safe, never loop blindly.

Canonical Controller Loop (Conceptual)
state = initialize_state()

while state["status"] != "stop":

    agent = select_next_agent(state)
    output = run_agent(agent, state)

    state = update_state(state, output)

    if should_stop(state):
        state["status"] = "stop"

State Update Rules
After Evaluation Agent
state["iteration"] += 1
state["verdict"] = evaluation_output["verdict"]
state["issue_type"] = evaluation_output.get("issue_type")

After Code Agent
state["issue_type"] = "execution" if execution_failed else None

After Design Agent
state["issue_type"] = "design" if spec_incomplete else None

Controller Guarantees

The controller guarantees:

âœ… Finite execution

âœ… Reproducible flow

âœ… Explicit termination

âœ… Zero hallucinated routing

âœ… Full auditability

Explicit Non-Responsibilities

The controller must never:

âŒ Inspect raw metrics

âŒ Interpret statistics

âŒ Modify experiment design

âŒ Modify code

âŒ Use an LLM

âŒ Decide scientific validity

Error Handling (Deterministic)
Invalid Agent Output
IF agent output schema invalid â†’ STOP + LOG ERROR

Repeated Failures
IF same issue_type repeats N times â†’ STOP


(N is fixed, e.g. 3)

Logging Requirements (Mandatory)

Each iteration logs:

{
  "iteration": 3,
  "agent_run": "evaluation",
  "verdict": "spurious",
  "issue_type": "design",
  "next_agent": "design"
}


This enables:

Replay

Debugging

Cost accounting

Why the Controller Is Deterministic (Rationale)

Routing decisions are rule-based

Inputs are structured

Output space is finite

LLMs add cost + nondeterminism

Determinism enables scientific reproducibility