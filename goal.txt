Autonomous Research System ‚Äì Agent Flow & Structure

## üéØ Goal of the System

Conduct **end-to-end research** on a topic:

- explore literature & ideas
- form hypotheses
- implement experiments (via Jupyter)
- analyze results
- iterate until quality threshold is met

---

## üß† Core Agents (WHO does WHAT)

### 1Ô∏è‚É£ Research Exploration Agent

**Role**

- **Build a high-quality research corpus** for future experiment execution
- **Targeted Information Gatherer**: Filter and search specifically for highly relevant papers and reports
- **Deep Dive & Filtered Ingest**: Download core PDFs and capture essential technical data (formulas, benchmarks) while excluding noise
- **No Inferencing**: Dedicated to building a lean, relevant information base without design or action responsibilities.

**Inputs**

- Research question / domain
- Previous iteration context (if any)

**Outputs**

- **Research Corpus Index**: A structured catalog of all ingested papers, notes, and technical snippets
- **Topic Summary**: A descriptive map of collected information and its location in memory
- **Raw Technical Data**: High-fidelity records available for downstream reasoning agents

**Tools**

- Web search (Tavily)
- PDF reader (download & extract text)
- Notes memory (save/search)

---

### 2Ô∏è‚É£ Experiment Design Agent

**Role**

- **Research-to-Experiment Translation**: Convert the Research Corpus into a concrete, executable experiment design.
- **Hypothesis & Variable Definition**: Formulate testable hypotheses (H‚ÇÄ, H‚ÇÅ), identify independent/dependent variables, and define controls.
- **Experimental & Statistical Specification**: Specify architectures, datasets, baselines, metrics, and the statistical analysis plan required for valid evaluation.
- **Strict Structured Output**: Produce a single valid JSON object with detailed specifications.

**Inputs**

- **User Research Goal** (Original prompt/intent).
- **Research Notes/Corpus** (List of strings/excerpts from the Research Agent).

**Outputs**

- **JSON Object** containing:
  - **Experiment Specification**:
    - Research question & Hypotheses (H‚ÇÄ, H‚ÇÅ)
    - Variables (independent, dependent, controls)
    - Model design & Dataset requirements
  - **Statistical Analysis Plan**:
    - Primary/Fallback tests, Alpha, Assumptions
  - **Power Analysis Plan**:
    - Method, Effect size, Target power
  - **Evidence Used**: References to the research notes used to justify decisions.

**Tools**

- **Corpus Query Tool**: Evidence-grounded retrieval over research notes (using TF-IDF) to justify experimental design choices.
- *(Note: Experimental logic, statistical selection, and power analysis specifications are handled via internal chain-of-thought/LLM reasoning, not external tools)*

---

### 3Ô∏è‚É£ Code & Execution Agent

#### Purpose
The **Code & Execution Agent** is responsible for the **deterministic realization** of an experiment defined by the **Experiment Design Agent**. It focuses strictly on **execution and data resolution**, making no scientific, statistical, or interpretive decisions.

---

#### Role

##### 1. Deterministic Implementation
- **Authoritative Translation**: Convert the **Experiment Specification (JSON)** into executable artifacts without deviation.
- **Specification Loyalty**: Treat the specification as authoritative and immutable; implement *exactly* what is defined.

##### 2. Dataset Resolution (Execution-Level)
- **Constraint Satisfaction**: Resolve dataset requirements into a concrete instance (e.g., HF dataset or local path).
- **Validation**: Select a dataset *only* if it strictly satisfies all scientific constraints (sample size, task type, etc.).
- **Traceability**: Log resolved dataset metadata (commit hash, version, counts) for reproducibility.

##### 3. Autonomous Execution
- **Artifact Generation**: Produce `run_experiment.py` (default) or `.ipynb` notebooks (if explicitly requested).
- **Runtime Management**: Execute artifacts autonomously using a controlled runtime environment.
- **Iteration Support**: Support repeated execution strictly when directed by Evaluation Agent feedback.

##### 4. Evidence Generation (Raw Data)
- **Artifact Production**: Persist raw experimental evidence to disk in structured, machine-readable formats.
- **Data Integrity**: Ensure all outputs are reproducible, timestamped, and traceable.

---

#### Inputs

- **Experiment Specification (JSON)**: The single source of truth for research questions, hypotheses, variables, model design, and dataset requirements.
- **Statistical Analysis Plan**: Grounding for identifying what specific metrics and granularity (e.g., per-sample vs aggregate) must be logged.
- **Revision Directives (JSON)**: Optional instructions from the Evaluation Agent for additional runs or logging adjustments.

---

#### Outputs

- **Primary Artifacts**:
  - `run_experiment.py`: The executable experiment logic.
  - `dataset_used.json`: Resolution record (name, version, sample counts, verification).
  - `raw_results.{json|csv}`: Per-run, per-seed metrics ONLY (no aggregation).
  - `execution.log`: Full runtime telemetry and error traces.
- **Secondary Artifacts**:
  - **Descriptive Visualizations**: Loss curves and metric trajectories (strictly descriptive).
  - **Model Checkpoints**: Saved *only* if explicitly specified in the design.

---

#### Capabilities & Infrastructure

- **Execution Environment**: Python runtime with deterministic execution (explicit seed control) and CPU/GPU orchestration.
- **Machine Learning Stack**: Access to PyTorch, NumPy, and Dataset loaders (HuggingFace, torchvision, etc.).
- **File System Access**: Persistent storage and retrieval of all experiment artifacts.
- **Runtime Libraries**: `python`, `torch`, `numpy`, `json`, `logging`, `nbclient` (optional).

---

#### Operational Constraints (LOCKED)

- ‚ùó **No Design Authority**: Never modify hypotheses, variables, or scientific requirements.
- ‚ùó **No Statistical Authority**: Never compute p-values, perform hypothesis tests, or aggregate results.
- ‚ùó **No Interpretation**: Never judge experiment quality or suggest improvements.
- ‚ùó **Halt on Ambiguity**: If the specification is incomplete or inconsistent, must stop execution and await clarification.

---

#### Mental Model: The Lab Technician
- **Experiment Design Agent** ‚Üí Scientist (Reasoning)
- **Code & Execution Agent** ‚Üí Lab Technician (Deterministic Action)
- **Evaluation Agent** ‚Üí Reviewer (Validation)

---
### 4Ô∏è‚É£ Evaluation / Analysis Agent

**Role**

- **Enforce Scientific Rigor**: Critically evaluate results using statistical methods.
- **Statistical Execution**: Execute the tests defined by the Design Agent.
- **Validation & Decision**: Judge **quality, correctness, and robustness** of results.
- Recommend the **next research action** based on statistical confidence.

> This agent thinks, calculates, and judges ‚Äî it does not act or execute experiment code.

---

**Inputs**

- **Analysis Protocol** from the Design Agent.
- **Evaluation artifacts**
    - Raw Metrics & results
    - Plots / logs
    - Experiment summaries
- **Research context** (Problem statement, $H_0$/$H_1$)

---

**Outputs**

- **Statistical Verdict**:
    - Calculated **p-values, confidence intervals, and effect sizes**.
    - **Assumption Check**: Verification of normality, independence, and variance ($O(n^2)$ is not what we mean here, but checking for bias).
- **Quality Score / Verdict**: 
    - `Reject H‚ÇÄ` / `Fail to reject H‚ÇÄ`
    - Decisions: `robust`, `spurious`, `promising`, `failed`.
- **Decision recommendation**: iterate, refine, or stop.
- **Rationale**: Why the results are (or aren't) statistically significant.

---

**Tools**

- Evaluation rubric
    - correctness
    - empirical signal vs noise
    - theoretical consistency
    - reproducibility
- Threshold rules
    - minimum improvement
    - max iterations
    - diminishing returns detection
- Comparison memory
    - previous experiment results
    - trend tracking

---

## üß≠ Control Layer (HOW things flow)

### Controller (Python / LangGraph)

**Responsibilities**

- Maintain global state
- Route outputs between agents
- Enforce iteration limits
- Prevent infinite loops

**State Example**

```json
{
"iteration":3,
"hypothesis":"...",
"quality_score":0.71,
"max_iterations":8,
"status":"iterate"
}
```

‚ùó This is **NOT an agent**

‚ùó This is **deterministic control**

---

## üîÅ High-Level Flow (SEQUENCE)

1. **Research Exploration Agent**: Build corpus and technical index.
2. **Experiment Design Agent**: Translate research into formal JSON specification.
3. **Code & Execution Agent**: Implement and execute experiments in Jupyter.
4. **Evaluation & Analysis Agent**: Statistically validate results and decide next steps.
5. **Controller**: Decide whether to iterate (loop to 2 or 3) or terminate.

---

## üß∞ Tool Summary (WHAT is connected)

### LLM Tools

- OpenAI Agent SDK (for agents)
- Structured outputs (JSON)

### Execution Tools

- Jupyter Notebook kernel
- Python execution
- GPU access

### Data Tools

- Dataset loaders
- File system
- Experiment logs

### Memory

- Research notes store
- Experiment history store
- Failure patterns store